version: 2.1

dockerhub_auth: &dockerhub_auth
  auth:
    username: $DOCKERHUB_USERNAME
    password: $DOCKERHUB_PASSWORD

deploy_filter: &deploy_filter
  filters:
    branches:
      ignore:
        - /__.*/

commands:
  top_level_install:
    steps:
      - restore_cache:
          keys:
            - top-level-dependencies-{{ checksum "package-lock.json" }}-v2
      - run:
          command: |
            [ -e "node_modules" ] || npm ci
      - save_cache:
          key: top-level-dependencies-{{ checksum "package-lock.json" }}-v2
          paths:
            - node_modules
  form_backend_install:
    steps:
      - restore_cache:
          keys:
            - form-backend-dependencies-{{ checksum "form_backend/package-lock.json" }}-v2
      - run:
          command: |
            [ -e "form_backend/node_modules" ] || (cd form_backend && npm ci)
      - save_cache:
          key: form-backend-dependencies-{{ checksum "form_backend/package-lock.json" }}-v2
          paths:
            - form_backend/node_modules
  server_install:
    steps:
      - restore_cache:
          keys:
            - server-dependencies-{{ checksum "server/package-lock.json" }}-v2
      - run:
          command: |
            [ -e "server/node_modules" ] || (cd server && npm ci)
      - save_cache:
          key: server-dependencies-{{ checksum "server/package-lock.json" }}-v2
          paths:
            - server/node_modules
  client_install:
    steps:
      # need to bust the client package cache on patch changes in addition to package-lock changes
      - run: cksum ./client/patches/* > client-patch-checksums
      - restore_cache:
          keys:
            - client-dependencies-{{ checksum "client/package-lock.json" }}-{{ checksum "client-patch-checksums" }}-v2
      - run:
          command: |
            [ -e "client/node_modules" ] || (cd client && npm ci)
      - save_cache:
          key: client-dependencies-{{ checksum "client/package-lock.json" }}-{{ checksum "client-patch-checksums" }}-v2
          paths:
            - client/node_modules
            - client/.cache/Cypress # path set by CYPRESS_CACHE_FOLDER env var
  run_codecov:
    parameters:
      coverage_path:
        type: string
      flag:
        type: string
    steps:
      # downloads codecov to ./codecov, checksums, runs in project (assumes matching flag exists in codecov.yml
      - run:
          command: |
            curl -fLso codecov https://codecov.io/bash;
            VERSION=$(grep -o 'VERSION=\"[0-9\.]*\"' codecov | cut -d'"' -f2);
            for i in 1 256 512
            do
              sha${i}sum -c <(curl -s "https://raw.githubusercontent.com/codecov/codecov-bash/${VERSION}/SHA${i}SUM" | grep -w "codecov")
            done
            chmod +x codecov

            echo << parameters.coverage_path >> | xargs -n1 ./codecov -F << parameters.flag >> -f $1

jobs:
  static_analysis:
    docker:
      - image: eaadtbs/ib-ci:1.3
        <<: *dockerhub_auth
    working_directory: "~/InfoBase"
    steps:
      - checkout:
          path: "~/InfoBase"

      - top_level_install
      - form_backend_install
      - server_install
      - client_install

      # reminder: .eslintcache is rm-ed by postinstall/ci scripts, so restoring it must occur post-install!
      - restore_cache:
          keys:
            - repo-wide-lint-cache-{{ .Branch }}
            - repo-wide-lint-cache-master
      - run:
          command: npm run static_analysis
      - save_cache:
          key: repo-wide-lint-cache-{{ .Branch }}
          paths:
            - .eslintcache

  test_form_backend:
    docker:
      - image: eaadtbs/ib-ci:1.3
        <<: *dockerhub_auth
    working_directory: "~/InfoBase"
    steps:
      - checkout:
          path: "~/InfoBase"

      - top_level_install
      - form_backend_install

      - restore_cache:
          keys:
            - form-backend-jest-cache

      - run: (cd form_backend && npm run unit_tests)

      - run: (cd form_backend && npm run end_to_end_tests)

      - run_codecov:
          coverage_path: form_backend/.coverage/*/coverage-final.json
          flag: form_backend

      - save_cache:
          key: form-backend-jest-cache
          paths:
            - form_backend/.cache/jest

  cleanup_dev_links:
    docker:
      - image: eaadtbs/ib-ci:1.3
        <<: *dockerhub_auth
    working_directory: "~/InfoBase"
    steps:
      - checkout:
          path: "~/InfoBase"
      - run: ./scripts/ci_scripts/create_envs.sh

      # only let this job run in the true repo
      - run: if [ "$CIRCLE_PROJECT_REPONAME" != "infobase" ]; then circleci-agent step halt; fi

      - run:
          command: git branch -r > ./active-branches

      # Use checksum of branch list to tell if the dev DBs need to be cleaned up
      - restore_cache:
          keys:
            - dev-dbs-clean-for-these-branches--{{ checksum "./active-branches" }}-v2
      - run:
          command: |
            if [ ! -f "./dev-dbs-clean-for-these-branches" ]; then
              ./scripts/ci_scripts/cleanup_dev_dbs.sh
              ./scripts/ci_scripts/cleanup_dev_links.sh
            fi
      - run: touch ./dev-dbs-clean-for-these-branches
      - save_cache:
          key: dev-dbs-clean-for-these-branches--{{ checksum "./active-branches" }}-v2
          paths:
            - ./dev-dbs-clean-for-these-branches

  checkout_and_install:
    docker:
      - image: eaadtbs/ib-ci:1.3
        <<: *dockerhub_auth
    working_directory: "~/InfoBase"
    resource_class: medium
    steps:
      - checkout:
          path: "~/InfoBase"
      - run: ./scripts/ci_scripts/create_envs.sh

      - top_level_install
      - client_install
      - server_install

      - persist_to_workspace:
          root: ./
          paths:
            - ./*

  build_client:
    docker:
      - image: eaadtbs/ib-ci:1.3
        <<: *dockerhub_auth
    working_directory: "~/InfoBase"
    resource_class: medium
    steps:
      - attach_workspace:
          at: ./
      - run: ./scripts/ci_scripts/create_envs.sh

      # logging memory usage, build job has maxed out the memory available in CircleCI in the past, good to have this around if it does again
      - run:
          command: |
            while true; do
              sleep 5
              echo "====="
              ps -eo pgrp,comm,vsz,rss,%mem,cmd:1000 --sort %mem |\
                awk '{
                  if (NR==1) printf "%10s %20s %10s %10s %5s %s", $1, $2, $3, $4, $5, $6;
                  else printf "%10s %20s %10s %10s %5s", $1, $2, $3/1024, $4/1024, $5;
                  $1=$2=$3=$4=$5="";
                  print $0;
                }'
              echo "====="
            done
          background: true

      - run: ./scripts/ci_scripts/get_bundle_stats_baseline.sh

      - restore_cache:
          keys:
            # the trailing - means CircleCI will use the latest key matching up to that point
            - webpack-caches-v6-{{ .Branch }}-

      - restore_cache:
          keys:
            - prod-build-{{ .Branch }}{{ .Revision }}-v2
      - run:
          command: |
            [ -f "client/$BUILD_DIR/InfoBase/app/app-a11y-en.min.js" ] || (cd client && ./scripts/deploy/build_all.sh -c "half" -m 1700 -o "PROD_SOURCE_MAP ISTANBUL STATS $([[ $CIRCLE_BRANCH = master ]] && echo "STATS-BASELINE")")
      - save_cache:
          key: prod-build-{{ .Branch }}-{{ .Revision }}-v2
          paths:
            - client/build

      - save_cache:
          key: webpack-caches-v6-{{ .Branch }}-{{ .Revision }}
          paths:
            - client/node_modules/.cache/webpack
            - client/node_modules/.cache/babel-loader
            - client/.eslintcache

      # TODO persist_to_workspace only works on an allow list basis which is a bother to maintain, currently seems better (if not ideally
      # efficient) to persist everything with specific exclusions of large and unnecessary directories made via rm's in advance
      - run: rm -rf client/node_modules/.cache/webpack

      - persist_to_workspace:
          root: ./
          paths:
            - ./*

  check_gql_codegen_integrity:
    docker:
      - image: eaadtbs/ib-ci:1.3
        <<: *dockerhub_auth
    working_directory: "~/InfoBase"
    steps:
      - attach_workspace:
          at: ./
      - run: ./scripts/ci_scripts/create_envs.sh

      - run:
          name: server mongod
          command: (cd server && npm run mongod)
          background: true
      - run: (cd server && npm run populate_db)
      - run:
          name: start_api_server
          command: (cd server && npm run start)
          background: true

      - run: while true; do sleep 10; curl -f http://localhost:1337/.well-known/apollo/server-health || continue && break; done
      - run: (cd client && npm run gqlgen)

      - run: git status | grep "\.gql\.ts" && echo "Committed .gql.ts files do not match output of gqlgen in CI. Regenerate and commit these types locally." && exit 1

  test_client:
    docker:
      - image: eaadtbs/ib-ci:1.3
        <<: *dockerhub_auth
    working_directory: "~/InfoBase"
    resource_class: medium
    steps:
      - attach_workspace:
          at: ./
      - run: ./scripts/ci_scripts/create_envs.sh

      - restore_cache:
          keys:
            - client-jest-cache

      - run: (cd client && npm run unit_tests)

      - run_codecov:
          coverage_path: client/.coverage/unit_tests/coverage-final.json
          flag: client

      - save_cache:
          key: client-jest-cache
          paths:
            - client/.cache/jest

  test_server:
    docker:
      - image: eaadtbs/ib-ci:1.3
        <<: *dockerhub_auth
    working_directory: "~/InfoBase"
    steps:
      - attach_workspace:
          at: ./
      - run: ./scripts/ci_scripts/create_envs.sh

      - restore_cache:
          keys:
            - server-jest-cache

      - run: (cd server && npm run unit_tests)

      - run:
          name: server mongod
          command: (cd server && npm run mongod)
          background: true

      - run: (cd server && npm run snapshot_tests)

      - run_codecov:
          coverage_path: server/.coverage/*/coverage-final.json
          flag: server

      - save_cache:
          key: server-jest-cache
          paths:
            - server/.cache/jest

  test_end_to_end:
    docker:
      - image: eaadtbs/ib-ci-cypress:5.0
        <<: *dockerhub_auth
    working_directory: "~/InfoBase"
    parameters:
      batch-count:
        type: integer
        default: 1
      batch-index:
        type: integer
        default: 0
    steps:
      - attach_workspace:
          at: ./
      - run: ./scripts/ci_scripts/create_envs.sh

      - run:
          name: server mongod
          command: (cd server && npm run mongod)
          background: true
      - run: (cd server && npm run populate_db)
      - run:
          name: start_api_server
          command: (cd server && npm run start)
          background: true

      - run: sed -i "s#${CDN_URL}#.#g" ./client/$BUILD_DIR/InfoBase/app/*.js # Replace the CDN_URL instances in the built InfoBase so the tests will get the right files (ie. NOT just test the previous deploy)
      - run: sed -i "s#${CDN_URL}#.#g" ./client/$BUILD_DIR/InfoBase/*.html # Replace the CDN_URL instances in the built InfoBase so the tests will get the right files (ie. NOT just test the previous deploy)
      - run: sed -i "s#hacky_target_text_for_ci_to_replace_with_test_and_deploy_time_api_urls#http://127.0.0.1:1337/graphql#g" ./client/$BUILD_DIR/InfoBase/app/*.js # Replace CI API URL placeholder with local URL so these tests use the local server

      - run:
          name: start_http_server
          command: (cd client && npm run serve:ci)
          background: true

      - run: |
          shopt -s globstar && cat ./client/**/cypress-snapshots.js\
          | grep -vE '^\s*(//)'\
          > pre-run-cypress-snapshots-without-comments

      - run: (cd client && npm run cypress -- -e BATCH_COUNT=<< parameters.batch-count >>,BATCH_INDEX=<< parameters.batch-index >>)

      - run: touch post-run-cypress-snapshots && shopt -s globstar && cat ./client/**/cypress-snapshots.js >> post-run-cypress-snapshots
      - run: |
          diff pre-run-cypress-snapshots-without-comments post-run-cypress-snapshots\
          || (\
            echo "CI cypress runs SHOULD NOT result in changes to the snapshot file. This indicates that the snapshots need to be updated locally (exiting routes removed, new routes initialized... fiddly formating fixed, etc.)"\
            && exit 1\
          )

      - run_codecov:
          coverage_path: client/.coverage/cypress/coverage-final.json
          flag: e2e_shallow

      - run: circleci step halt # held open at this point by the running server, force halt (returns successful)

  deploy_data:
    working_directory: "~/InfoBase"
    docker:
      - image: eaadtbs/ib-ci:1.3
        <<: *dockerhub_auth
    steps:
      - attach_workspace:
          at: ./
      - run: ./scripts/ci_scripts/create_envs.sh

      # Checksum all files in data dir, used to determine if the database needs to be populated
      - run: cksum ./data/* > data-checksums

      # Use this cached file to tell if the data needs to be loaded to mongodb (if on a new branch or if data has changed)
      - restore_cache:
          keys:
            - data-is-deployed-{{ .Branch }}-{{ checksum "./data-checksums" }}-v2
      - run:
          command: |
            [ -f "./this-data-is-deployed" ] || (cd server && npm run populate_db:remote)
      - run: touch ./this-data-is-deployed
      - save_cache:
          key: data-is-deployed--{{ .Branch }}-{{ checksum "./data-checksums" }}-v2
          paths:
            - ./this-data-is-deployed

  deploy_server:
    working_directory: "~/InfoBase"
    docker:
      - image: google/cloud-sdk:slim
    steps:
      - attach_workspace:
          at: ./
      - run: ./scripts/ci_scripts/create_envs.sh

      - run: ./scripts/ci_scripts/authenticate-server-gcloud.sh

      # Checksum all JS files in src, used to determine if the function needs to be redeployed
      - run: find ./server/src -name "*.js" | xargs cksum > server-checksums

      # Use this cached file to tell if the api needs to be redeployed (if on a new branch, if function code's changed, or if server node modules have changed)
      - restore_cache:
          keys:
            - api-is-deployed-{{ .Branch }}-{{ checksum "./server-checksums" }}-{{ checksum "./server/package-lock.json" }}-v2
      - run:
          command: |
            [ -f "./this-api-is-deployed" ] || (cd server && ./scripts/deploy/ci_deploy_function.sh)
      - run: touch ./this-api-is-deployed
      - save_cache:
          key: api-is-deployed--{{ .Branch }}-{{ checksum "./server-checksums" }}-{{ checksum "./server/package-lock.json" }}-v2
          paths:
            - ./this-api-is-deployed

  deploy_client:
    working_directory: "~/InfoBase"
    docker:
      - image: google/cloud-sdk:slim
    steps:
      - attach_workspace:
          at: ./
      - run: ./scripts/ci_scripts/create_envs.sh
      - run: ./scripts/ci_scripts/authenticate-client-gcloud.sh

      # Replace CI API URL placeholder with branch specific API URL
      - run: sed -i "s#hacky_target_text_for_ci_to_replace_with_test_and_deploy_time_api_urls#https://us-central1-ib-serverless-api-dev.cloudfunctions.net/$CIRCLE_BRANCH/graphql#g" ./client/$BUILD_DIR/InfoBase/app/*.js
      - run: (cd client && ./scripts/deploy/push_to_gcloud_bucket.sh)

      # only saving master stats right now since they're somewhat large files (~1 mb each)
      - run: if [[ $CIRCLE_BRANCH = master ]]; then ./scripts/ci_scripts/store_bundle_stats.sh; fi

workflows:
  static_analysis:
    jobs:
      - static_analysis

  test_form_backend:
    jobs:
      - test_form_backend

  cleanup_dev_links:
    jobs:
      - cleanup_dev_links:
          <<: *deploy_filter

  build_test_deploy:
    jobs:
      - checkout_and_install

      - check_gql_codegen_integrity:
          requires:
            - checkout_and_install

      - test_client:
          requires:
            - checkout_and_install
      - build_client:
          requires:
            - test_client

      - test_server:
          requires:
            - checkout_and_install
      - deploy_data:
          requires:
            - test_server
          <<: *deploy_filter
      - deploy_server:
          requires:
            - test_server
          <<: *deploy_filter

      - test_end_to_end:
          requires:
            - test_server
            - build_client
          matrix:
            parameters:
              batch-count: [3]
              batch-index: [0, 1, 2]

      - deploy_client:
          requires:
            - deploy_data
            - deploy_server
            - test_end_to_end
          <<: *deploy_filter
